{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/normal-depth-diffusion-colab/blob/main/normal_depth_diffusion_colab.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b dev https://github.com/camenduru/normal-depth-diffusion\n",
        "%cd /content/normal-depth-diffusion\n",
        "!pip install -q modelscope\n",
        "!python tools/download_models/download_nd_models.py\n",
        "!pip install -q diffusers invisible-watermark omegaconf pytorch_lightning taming-transformers-rom1504 kornia open-clip-torch\n",
        "!pip install -q webdataset\n",
        "!pip install -q git+https://github.com/openai/CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%env ckpt_path=/content/normal-depth-diffusion/pretrained_models/Damo_XR_Lab/Normal-Depth-Diffusion-Model/nd-laion_ema.ckpt\n",
        "%env prompt=\"a living room with a sofa\"\n",
        "%env save_dir=/content/normal-depth-diffusion/outputs/nd\n",
        "!python ./scripts/t2i.py --ckpt $ckpt_path --prompt \"$prompt\" --dpm_solver --n_samples 2 --save_dir $save_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%env ckpt_path=/content/normal-depth-diffusion/pretrained_models/Damo_XR_Lab/Normal-Depth-Diffusion-Model/nd_mv_ema.ckpt\n",
        "%env prompt=\"a cute girl\"\n",
        "%env save_dir=/content/normal-depth-diffusion/outputs/nd-mv\n",
        "!python ./scripts/t2i_mv.py --ckpt_path $ckpt_path --prompt \"$prompt\" --num_frames 4 --model_name nd-mv --save_dir $save_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%env ckpt_path=/content/normal-depth-diffusion/pretrained_models/Damo_XR_Lab/Normal-Depth-Diffusion-Model/albedo_mv_ema.ckpt\n",
        "%env prompt=\"a wooden chair\"\n",
        "%env save_dir=/content/normal-depth-diffusion/outputs/albedo-mv\n",
        "%env depth_file=/content/normal-depth-diffusion/loads/chair_depth.png\n",
        "!python ./scripts/td2i_mv.py --ckpt_path $ckpt_path --prompt \"$prompt\" --depth_file $depth_file --num_frames 4 --model_name albedo-mv --save_dir $save_dir"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
